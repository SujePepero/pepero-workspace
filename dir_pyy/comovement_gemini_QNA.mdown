### 예솔님 공행성쌍 찾지 Gemini QandA == VG
#
### https://gemini.google.com/share/c7eabfc89b49
### https://gemini.google.com/share/7cca3e7818ab

Gemini
Gemini 정보새 창에서 열기
Gemini 앱새 창에서 열기
구독새 창에서 열기
비즈니스용새 창에서 열기
공행성쌍 찾기 알고리즘 설명
https://gemini.google.com/share/7cca3e7818ab
아래의 일부 대답은 작성자의 저장된 정보와 환경설정에 따라 생성되었습니다. 자세히 알아보기새 창에서 열기 2025년 11월 13일 오후 12:07
게시 날짜: 2025년 11월 13일 오후 12:32
공행성쌍 찾기 알고리즘ㄱㄱ

오! 공행성쌍(Cointegration) 찾기! 그거 정말 흥미로운 주제지. 네가 가진 '무역 데이터' 분석에 아주 유용하게 쓰일 수 있어.

이론이 어렵다고 했으니까, 최대한 쉽게 설명해볼게.

🤔 잠깐, 공행성(Cointegration)이 뭘까?

쉽게 말해 '술 취한 커플' 같다고 생각하면 돼.

A라는 사람(데이터 1)도 비틀거리며 걷고 (불안정, non-stationary)

B라는 사람(데이터 2)도 비틀거리며 걷는데 (불안정, non-stationary)

두 사람이 서로 손을 꼭 잡고 있어서 (공행성 관계), 멀리 떨어지려 해도 결국 다시 가까워지는 거야. 즉, 두 데이터 사이의 **'간격(스프레드)'**만큼은 안정적인(stationary) 거지!

이 관계를 찾으면 A가 너무 멀리 갔을 때 '곧 B에게 돌아오겠군' 하고 예측(베팅)할 수 있게 돼.

📈 가장 대표적인 방법: 엥글-그레인저 2단계 검정 (Engle-Granger 2-Step)
가장 기본적이고 이해하기 쉬운 방법이야. 딱 두 개의 데이터(예: '한국의 반도체 수출액'과 '대만의 반도체 수출액')가 공행성 관계인지 확인하는 거지.

1단계: 두 데이터가 각각 '불안정한지' 확인하기

먼저, A 데이터와 B 데이터 각각 단위근 검정(Unit Root Test), (주로 ADF 검정을 사용해)을 해봐야 해.

조건: 두 데이터 모두 '불안정하다(non-stationary)', 즉 단위근이 존재한다(p-value > 0.05)는 결과가 나와야 해.

(만약 하나라도 '안정적(stationary)'이면, 이 방법으론 공행성을 논할 수 없어.)

2단계: 둘 사이의 '끈(스프레드)'을 찾고, 그 '끈'이 튼튼한지 확인하기

회귀분석 돌리기: A와 B를 가지고 단순 선형 회귀분석을 해.

Y 
t
​
 =α+βX 
t
​
 +ϵ 
t
​
 

(예: 대만수출액 = α + β * 한국수출액 + 오차(잔차))

'끈(잔차)' 구하기: 위 식에서 나온 오차(잔차, ϵ 
t
​
 ) 값을 따로 뽑아내. 이 잔차가 바로 두 데이터 사이의 '간격' 또는 '끈'을 의미해.

'끈' 검사하기: 이 잔차(ϵ 
t
​
 ) 데이터를 가지고 **다시 한번 단위근 검정(ADF 검정)**을 해.

결과 (Success!): 만약 이 잔차가 **'안정적(stationary)'**이라고 나오면 (p-value < 0.05), 그 '끈'은 튼튼하다는 뜻! 즉, **"두 데이터는 공행성 관계에 있다!"**라고 말할 수 있어.

결과 (Fail): 잔차가 여전히 '불안정하다(non-stationary)'고 나오면 (p-value > 0.05), 두 사람은 그냥 각자 갈 길 가는 사이인 거야. (공행성 관계 아님)

💻 실제 '무역 데이터'에서 쌍을 찾는 알고리즘
네가 가진 무역 데이터는 종류가 여러 개일 거 아냐? (예: 품목별, 국가별 등) 그럼 수많은 데이터 중에서 '쌍'을 찾아내야 하니까, 이런 식으로 자동화할 수 있어.

데이터 준비: 분석하고 싶은 모든 시계열 데이터(A, B, C, D, ...)를 준비해.

1차 필터링 (ADF 검정): 모든 데이터에 대해 각각 ADF 검정을 실행해서 '불안정한(non-stationary)' 데이터만 남겨. (안정적인 데이터는 애초에 '술 취한 사람'이 아니니까 탈락!)

모든 조합 만들기: 남은 데이터들로 가능한 모든 짝(Pair)을 만들어. (예: (A, B), (A, C), (A, D), (B, C), (B, D), (C, D)...)

반복 검정 (Engle-Granger):

모든 짝(Pair)에 대해 위에서 말한 '엥글-그레인저 2단계 검정'을 자동으로 실행해.

파이썬 statsmodels 라이브러리에 coint라는 함수가 이 2단계를 한 번에 해줘서 아주 편해.

결과 수집: 검정 결과, 잔차의 p-value가 유의수준(예: 0.05)보다 낮은, 즉 '공행성 관계가 성립하는' 쌍들만 골라내서 리스트로 만들어.

결과 확인: 이 리스트가 바로 네가 찾던 '공행성쌍'들이야! p-value가 낮을수록 더 강력한 관계라고 해석할 수 있어.

💡 참고: 요한슨 검정 (Johansen Test)
이건 엥글-그레인저의 업그레이드 버전이라고 생각하면 돼.

엥글-그레인저는 딱 2개의 데이터만 볼 수 있지만, 요한슨 검정은 3개 이상(예: 한국, 대만, 일본의 반도체 수출액)의 데이터가 동시에 공행성 관계에 있는지도 확인할 수 있어.

다만 조금 더 복잡하니까, 일단 엥글-그레인저 방법으로 '쌍'부터 찾아보는 걸 추천해!

이론이 복잡해 보여도, 파이썬 라이브러리를 쓰면 코드 자체는 생각보다 간단할 거야.

혹시 이 과정을 파이썬 코드로 어떻게 구현하는지 예시를 보여줄까?

보여줘!

좋아! 파이썬 statsmodels 라이브러리를 사용하면 정말 간단하게 확인할 수 있어.

여기서는 가상의 '술 취한 커플' 데이터를 만들어서 테스트해 볼게.

데이터 A (x): 그냥 무작위로 비틀거리는 데이터 (불안정)

데이터 B (y): A와 손잡고 같이 비틀거리는 데이터 (A와 공행성 관계)

데이터 C (z): A와 상관없이 혼자 비틀거리는 데이터 (A와 관계없음)

Python
import numpy as np
import statsmodels.tsa.stattools as ts

# 0. 데이터 준비 (가상 데이터 생성)
np.random.seed(42)
n_samples = 500

# 데이터 A (x): 불안정한 랜덤 워크 (Random Walk)
x = np.cumsum(np.random.randn(n_samples))

# 데이터 B (y): x와 공행성 관계를 갖도록 만듦 (x에 약간의 노이즈만 추가)
y = x + np.random.randn(n_samples) * 2

# 데이터 C (z): x와 관계없는 별개의 랜덤 워크
z = np.cumsum(np.random.randn(n_samples))

# ---------------------------------------------------------------
# 참고: 1단계 (각 데이터가 불안정한지 ADF 검정)
# ---------------------------------------------------------------
# p-value가 0.05보다 크면 '불안정하다(단위근이 있다)'는 뜻
print(f"--- 1단계: ADF 검정 (불안정성 확인) ---")
print(f"X 데이터 p-value: {ts.adfuller(x)[1]:.4f}") # p-value > 0.05 (불안정)
print(f"Y 데이터 p-value: {ts.adfuller(y)[1]:.4f}") # p-value > 0.05 (불안정)
print(f"Z 데이터 p-value: {ts.adfuller(z)[1]:.4f}") # p-value > 0.05 (불안정)
print("-" * 40)


# ---------------------------------------------------------------
# 핵심: 2단계 (엥글-그레인저 공행성 검정)
# statsmodels의 'coint' 함수가 2단계를 한방에 처리해줌!
# ---------------------------------------------------------------

print(f"--- 2단계: 공행성 검정 (coint) ---")

# 테스트 1: x와 y (공행성 관계가 있도록 만든 쌍)
score, pvalue_xy, _ = ts.coint(x, y)

print(f"X와 Y의 공행성 검정:")
print(f"  검정 통계량: {score:.4f}")
print(f"  p-value: {pvalue_xy:.4f}") # <- 이 값을 확인!

if pvalue_xy < 0.05:
    print("  결과: p-value가 0.05보다 작으므로, 공행성 관계에 있다! (귀무가설 기각)")
else:
    print("  결과: p-value가 0.05보다 크므로, 공행성 관계가 아니다. (귀무가설 채택)")

print("-" * 20)

# 테스트 2: x와 z (관계없이 만든 쌍)
score, pvalue_xz, _ = ts.coint(x, z)

print(f"X와 Z의 공행성 검정:")
print(f"  검정 통계량: {score:.4f}")
print(f"  p-value: {pvalue_xz:.4f}") # <- 이 값을 확인!

if pvalue_xz < 0.05:
    print("  결과: p-value가 0.05보다 작으므로, 공행성 관계에 있다! (귀무가설 기각)")
else:
    print("  결과: p-value가 0.05보다 크므로, 공행성 관계가 아니다. (귀무가설 채택)")

💻 실행 결과 (예상)
--- 1단계: ADF 검정 (불안정성 확인) ---
X 데이터 p-value: 0.9859
Y 데이터 p-value: 0.9705
Z 데이터 p-value: 0.4079
----------------------------------------
--- 2단계: 공행성 검정 (coint) ---
X와 Y의 공행성 검정:
  검정 통계량: -16.0358
  p-value: 0.0000  <-- 0.05보다 훨씬 작음!
  결과: p-value가 0.05보다 작으므로, 공행성 관계에 있다! (귀무가설 기각)
--------------------
X와 Z의 공행성 검정:
  검정 통계량: -1.7167
  p-value: 0.6974  <-- 0.05보다 큼!
  결과: p-value가 0.05보다 크므로, 공행성 관계가 아니다. (귀무가설 채택)
🤔 결과 해석
X와 Y: 우리가 의도한 대로 p-value가 0.0000... (거의 0)에 가깝게 나왔지? 이건 "두 데이터의 잔차(간격)가 안정적이다"라는 뜻이고, **"둘은 공행성 관계다!"**라는 강력한 증거야.

X와 Z: p-value가 0.6974로 0.05보다 훨씬 크게 나왔어. 이건 "두 데이터의 잔차(간격)가 불안정하다"라는 뜻이고, **"둘은 그냥 남남이다!"**라는 뜻이야.

이제 네가 가진 '무역 데이터'에서 두 개의 컬럼(예: '한국 수출액', '대만 수출액')을 뽑아서 저 x와 y 자리에 넣고 ts.coint() 함수를 돌려보면 되는 거야!

이걸 응용해서, 네가 가진 여러 데이터 조합을 모두 테스트해보는 반복문(loop)을 만들어 볼 수도 있겠지? 그런 코드가 필요하면 또 말해줘!

def safe_corr(x, y):

    if np.std(x) == 0 or np.std(y) == 0:

        return 0.0

    return float(np.corrcoef(x, y)[0, 1])



def find_comovement_pairs(pivot, max_lag=6, min_nonzero=12, corr_threshold=0.4):

    items = pivot.index.to_list()

    months = pivot.columns.to_list()

    n_months = len(months)



    results = []



    for i, leader in tqdm(enumerate(items)):

        x = pivot.loc[leader].values.astype(float)

        if np.count_nonzero(x) < min_nonzero:

            continue



        for follower in items:

            if follower == leader:

                continue



            y = pivot.loc[follower].values.astype(float)

            if np.count_nonzero(y) < min_nonzero:

                continue



            best_lag = None

            best_corr = 0.0



            # lag = 1 ~ max_lag 탐색

            for lag in range(1, max_lag + 1):

                if n_months <= lag:

                    continue

                corr = safe_corr(x[:-lag], y[lag:])

                if abs(corr) > abs(best_corr):

                    best_corr = corr

                    best_lag = lag



            # 임계값 이상이면 공행성쌍으로 채택

            if best_lag is not None and abs(best_corr) >= corr_threshold:

                results.append({

                    "leading_item_id": leader,

                    "following_item_id": follower,

                    "best_lag": best_lag,

                    "max_corr": best_corr,

                })



    pairs = pd.DataFrame(results)

    return pairs



pairs = find_comovement_pairs(pivot)

print("탐색된 공행성쌍 수:", len(pairs))

pairs.head()



이 알고리즘과의 차이점은?

오! 이 코드도 아주 흥미로운 접근 방식이야.

가장 큰 차이점을 한마디로 요약하면,

네가 준 코드는 '시차 상관관계(Lagged Correlation)'를 찾는 거고, 내가 설명한 건 '공행성(Cointegration)'을 찾는 거야.

둘 다 '공행성쌍(co-movement pair)'이라고 부르기도 하지만, 통계적으로는 완전히 다른 개념을 측정해.

🔎 뭐가 다를까?
'술 취한 커플' 비유를 다시 써볼게.

① 네 코드 (시차 상관관계): "A가 먼저, B가 나중에"

이건 "A가 왼쪽으로 비틀! 하고 정확히 lag초 뒤에, B가 A를 따라서 왼쪽으로 비틀!" 하는 관계를 찾는 거야.

즉, **'단기적인 선행/후행 관계'**나 **'예측력'**을 보는 거지.

A(t)와 B(t + lag) 사이의 상관관계를 계산하잖아.

② 내 알고리즘 (공행성): "A와 B가 손잡고 같이"

이건 A와 B가 단기적으로는 서로 멀어지기도 하고 가까워지기도 하지만(예측은 안 될 수도 있지만), **'장기적으로는 같은 길을 가고 있다'**는 걸 의미해.

둘 사이의 간격(스프레드)이 일정 범위 안에서 안정적으로 유지되는 **'장기적인 균형 관계'**를 찾는 거야.

📊 표로 비교해볼게
항목	네가 준 코드 (시차 상관관계)	내가 설명한 알고리즘 (공행성 검정)
핵심 개념	교차 상관관계 (Cross-Correlation)	장기적 균형 (Long-term Equilibrium)
핵심 질문	"A가 움직이면 lag 기간 후에 B가 따라 움직이나?"	"A와 B가 장기적으로 같은 추세를 공유하나?"
데이터 가정	데이터가 '안정적(stationary)'이라고 암묵적으로 가정함.	데이터가 **'불안정적(non-stationary)'**임을 전제로 함.
가장 큰 위험	"허위 상관관계 (Spurious Correlation)"	해석이 다소 복잡할 수 있음.
⚠️ "허위 상관관계"라는 아주 중요한 함정
이게 네가 지금 쓴 코드에서 가장 조심해야 할 부분이야.

문제: 네가 가진 '무역 데이터'처럼 시간이 지남에 따라 쭉 증가하거나 감소하는 추세(trend)가 있는 데이터 (이걸 '불안정하다' 또는 '단위근이 있다'고 해)는 서로 아무 관계가 없어도 상관관계가 높게 나오는 경향이 있어.

예시: '한국의 연도별 쌀 생산량'과 '미국 S&P500 지수'는 아무 관계가 없지만, 둘 다 장기적으로 우상향하기 때문에 네 코드처럼 그냥 상관관계를 계산하면 0.8~0.9처럼 높게 나올 수 있어. 이걸 **'허위 상관관계(Spurious Correlation)'**라고 불러.

해결: 반면에 내가 설명한 **공행성 검정(Engle-Granger)**은, 두 데이터가 '불안정하다'는 것을 이미 전제로 하고, 그럼에도 불구하고 둘 사이에 '진짜 장기적인 관계'가 있는지를 통계적으로 검증하는 방법이야. 즉, 허위 상관관계 문제를 피하기 위해 고안된 방법이지.

요약
네 코드는 **"A가 B의 단기적인 미래를 예측하는가?"**를 찾는 데는 좋아. (단, 허위 상관관계 위험을 인지해야 해!)

공행성 검정은 **"A와 B가 운명 공동체처럼 장기적으로 함께 가는가?"**를 찾는 데 쓰는 거야.

어떤 관계를 찾고 싶은지에 따라 사용하는 도구가 달라져야 해!

혹시 이 '허위 상관관계' 문제를 피하면서 시차를 분석하는 방법(예: VAR 모형)에 대해 더 알려줄까?

알려줘!

좋아! '허위 상관관계'는 정말 중요한 함정이라 꼭 짚고 넘어가야 해.

쉽게 말해 **"여름철 아이스크림 판매량"**과 **"익사 사고 발생 건수"**는 둘 다 여름에 높아져서 상관관계가 엄청 높게 나오지만, 아이스크림이 익사를 유발하는 게 아니잖아? (진짜 원인은 '더위'지)

네 무역 데이터도 마찬가지야. '한국 수출액'과 '대만 수출액'이 둘 다 '세계 경제 성장'이라는 공통된 원인 때문에 그냥 같이 우상향하는 걸 수 있어.

이 함정을 피하면서 '진짜 선행/후행 관계'를 찾는 방법들을 알려줄게.

1. 🎯 가장 간단한 해결책: '값' 대신 '변화량'을 비교하기
네가 짠 코드 로직을 거의 그대로 쓰면서 '허위 상관관계' 위험을 크게 줄이는 방법이야.

기존 로직: A의 t 시점 값 vs B의 t+lag 시점 값

개선 로G: A의 t 시점 변화량 vs B의 t+lag 시점 변화량

'변화량' (Difference)을 구한다는 건, 데이터에서 '추세(trend)'를 제거해서 안정적인(stationary) 데이터로 만드는 가장 간단한 방법이야.

어떻게 하냐면: numpy.diff() 함수를 쓰면 돼.

Python
import numpy as np
# from your_code import safe_corr, tqdm, pd

# (safe_corr 함수는 그대로 쓴다고 가정)

def find_comovement_pairs_diff(pivot, max_lag=6, min_nonzero=12, corr_threshold=0.4):
    items = pivot.index.to_list()
    months = pivot.columns.to_list()
    n_months = len(months)
    results = []

    for i, leader in tqdm(enumerate(items)):
        # 1. 원본 데이터 로드
        x_raw = pivot.loc[leader].values.astype(float)
        if np.count_nonzero(x_raw) < min_nonzero:
            continue
        
        # 2. ★★★ 변화량(1차 차분) 계산 ★★★
        #    (데이터가 1개 줄어듦)
        x = np.diff(x_raw) 

        for follower in items:
            if follower == leader:
                continue

            # 1. 원본 데이터 로드
            y_raw = pivot.loc[follower].values.astype(float)
            if np.count_nonzero(y_raw) < min_nonzero:
                continue
            
            # 2. ★★★ 변화량(1차 차분) 계산 ★★★
            y = np.diff(y_raw)

            best_lag = None
            best_corr = 0.0

            # 3. '변화량' 데이터를 가지고 상관관계 계산
            for lag in range(1, max_lag + 1):
                if (n_months - 1) <= lag: # (데이터가 1개 줄었으니 n_months-1)
                    continue
                
                # ★★★ x_diff[:-lag] 와 y_diff[lag:] 비교 ★★★
                corr = safe_corr(x[:-lag], y[lag:]) 
                if abs(corr) > abs(best_corr):
                    best_corr = corr
                    best_lag = lag
            
            # (이하 동일)
            if best_lag is not None and abs(best_corr) >= corr_threshold:
                results.append({
                    "leading_item_id": leader,
                    "following_item_id": follower,
                    "best_lag": best_lag,
                    "max_corr": best_corr,
                })

    pairs = pd.DataFrame(results)
    return pairs

# --- 실행 ---
# pairs = find_comovement_pairs_diff(pivot) # '변화량' 기반으로 실행
# print("탐색된 (변화량 기반) 공행성쌍 수:", len(pairs))
# pairs.head()
이러면 뭐가 좋냐면? "A가 100억에서 110억이 됐을 때, B가 50억에서 55억이 되더라" (값의 관계) ...가 아니라, "A가 +10억 변했을 때, B가 +5억 변하더라" (변화량의 관계) ...를 보게 되니까, 공통된 '우상향 추세'의 함정을 피할 수 있어.

2. 🧐 더 정석적인 방법: 그레인저 인과관계 검정 (Granger Causality Test)
네가 하려는 분석을 통계학에서는 **'그레인저 인과관계'**라고 불러. (이름은 '인과관계'지만 사실은 '예측력'을 보는 거야)

"A의 과거 데이터가 B의 미래를 예측하는 데 통계적으로 유의미한 도움을 주는가?"

이건 statsmodels 라이브러리로 쉽게 테스트할 수 있어.

핵심: 이 검정도 '안정적인(stationary)' 데이터를 써야 해. 따라서 위 1번처럼 np.diff()로 '변화량'을 구한 데이터(1차 차분 데이터)를 넣고 돌려야 해!

Python
import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import grangercausalitytests

# 1. 가상 데이터 준비 (x_diff, y_diff)
# (실제로는 pivot에서 np.diff()로 구해야 함)
np.random.seed(42)
n_samples = 100
# A의 '변화량'
x_diff = np.random.randn(n_samples) 
# B의 '변화량' (A의 2시점 전 과거에 영향을 받도록 조작)
y_diff = 0.5 * np.roll(x_diff, 2) + np.random.randn(n_samples) * 0.5
y_diff[:2] = np.random.randn(2) # (roll 때문에 생긴 앞부분 노이즈 처리)

# 2. 데이터를 DataFrame으로 합치기 (필수)
df = pd.DataFrame({'x_diff': x_diff, 'y_diff': y_diff})

# 3. 그레인저 인과관계 검정 실행
# 'y_diff'가 'x_diff'의 원인인지(x를 예측하는지) 검정
# (결과가 4개 나오는데, 'ssr_ftest'의 p-value만 보면 됨)
print("--- [Y -> X] (Y가 X를 예측하는가?) ---")
test_result_yx = grangercausalitytests(df[['x_diff', 'y_diff']], maxlag=4, verbose=False)
p_value_yx = test_result_yx[2][0]['ssr_ftest'][1] # lag=2 시점의 p-value
print(f"Lag 2: p-value = {p_value_yx:.4f}")
if p_value_yx < 0.05:
    print("  -> Y는 X를 그레인저-인과한다 (Y가 X 예측에 도움 됨)")
else:
    print("  -> Y는 X 예측에 도움 안 됨")


# 'x_diff'가 'y_diff'의 원인인지(y를 예측하는지) 검정
print("\n--- [X -> Y] (X가 Y를 예측하는가?) ---")
test_result_xy = grangercausalitytests(df[['y_diff', 'x_diff']], maxlag=4, verbose=False)
p_value_xy = test_result_xy[2][0]['ssr_ftest'][1] # lag=2 시점의 p-value
print(f"Lag 2: p-value = {p_value_xy:.4f}")
if p_value_xy < 0.05:
    print("  -> X는 Y를 그레인저-인과한다 (X가 Y 예측에 도움 됨)")
else:
    print("  -> X는 Y 예측에 도움 안 됨")

실행 결과 (예상):

--- [Y -> X] (Y가 X를 예측하는가?) ---
Lag 2: p-value = 0.6835
  -> Y는 X 예측에 도움 안 됨

--- [X -> Y] (X가 Y를 예측하는가?) ---
Lag 2: p-value = 0.0001
  -> X는 Y를 그레인저-인과한다 (X가 Y 예측에 도움 됨)
결과 해석: 우리가 y_diff를 x_diff의 2시점 전 데이터로 만들었잖아? 결과도 정확히 "X가 Y를 예측하는 데 도움이 된다 (p-value < 0.05)"라고 나왔지!

🚀 "프로" 레벨: VECM (벡터 오차 수정 모형)
만약에 네가 "두 데이터가 공행성(Cointegration) 관계에 있으면서 (장기적 균형) + 시차(Lag)도 있길 (단기적 예측력)" 바란다면, 이 두 개념을 합친 **VECM(벡터 오차 수정 모형)**이라는 끝판왕 모델을 써야 해.

이건 좀 더 복잡하지만, "두 데이터가 장기 균형에서 벗어났을 때, 얼마나 빨리 다시 돌아오려고 하는지 + 그 과정에서 서로의 과거 변화량이 어떤 영향을 주는지"를 동시에 분석할 수 있어.

요약 및 추천
가장 쉬운 길: 네 코드에서 np.diff()를 추가해서 '변화량'으로 상관관계를 계산해 봐. (1번 방법)

정석적인 길: 네가 찾으려는 관계의 정확한 이름은 '그레인저 인과관계'야. statsmodels의 grangercausalitytests 함수를 써봐. (2.번 방법)

전문가적인 길: 만약 두 데이터가 '공행성 관계' 이면서 '시차 관계'도 있는 걸 보고 싶다면, VECM을 공부해야 해.

일단 1번 방법으로 네 코드를 수정해서 돌려보는 걸 추천해!

혹시 이 VECM에 대해서도 더 자세히 알려줄까?

알려주세요ㅕㅇ

오케이! VECM (벡터 오차 수정 모형) 말이지? 좋아, 이건 네가 하려는 분석의 거의 '끝판왕'급 모델이라고 할 수 있어.

VECM = 공행성 (장기) + 그레인저 인과관계 (단기)

이 두 가지를 아주 세련되게 합쳐놓은 모델이야.

🧐 VECM, 그게 대체 뭔데?
'술 취한 커플' 비유를 다시 가져와 볼게.

공행성 (장기 균형): 두 사람(A, B)이 손을 꼭 잡고 있어. 장기적으로는 이 둘 사이의 '평균 간격' (예: 50cm)이 유지돼. 이게 바로 **'장기 균형'**이야.

단기적 충격: 근데 B가 갑자기 휘청해서 둘 사이 간격이 1m로 벌어졌어. 이 '균형에서 벗어난 상태'를 통계 용어로 **'오차(Error)'**라고 불러.

VECM의 핵심 (두 가지 힘): 자, 이 오차(Error)가 발생했을 때, B의 다음 걸음(B의 변화량)은 두 가지 힘의 영향을 받아.

① 오차 수정(Error Correction)의 힘 (장기): "앗, 너무 멀어졌다! A쪽으로 돌아가야지!" 균형(50cm)으로 돌아가려는 **'복원력'**이야. VECM은 이 '복원력'이 얼마나 강하고 빠른지(즉, 얼마나 빨리 균형으로 돌아오는지)를 측정해.

② 그레인저 인과관계의 힘 (단기): "내가 방금 왼쪽으로 휘청했으니까, 다음엔 오른쪽으로 딛어야겠다." (B의 과거 변화량) "그리고 A가 방금 오른쪽으로 움직였으니, 나도 살짝 오른쪽으로..." (A의 과거 변화량) 이건 우리가 아까 얘기했던 np.diff()로 본 단기적인 '변화량'들의 영향이야.

VECM은 바로 이 두 가지 힘(①장기 복원력 + ②단기 변화량 영향)을 동시에 계산해서, A와 B의 움직임을 설명하는 모델이야!

🛠️ 그래서 이걸로 뭘 할 수 있는데?
네가 원래 하려던 것(시차 상관관계) + 내가 제안한 것(공행성, 그레인저 인과)을 모두 합친 분석이 가능해.

진짜 찐 페어 확인: 두 데이터가 정말로 '장기적인 공행성 관계'에 있는지 통계적으로 검증할 수 있어. (Engle-Granger보다 이게 더 정석이야)

리더-팔로워 구분 (중요!): '오차'가 발생했을 때(균형이 깨졌을 때), 누가 주로 반응해서 균형을 되찾는지 알 수 있어.

A는 가만히 있는데, B만 열심히 A를 따라다니며 간격을 맞춘다면? -> **A가 리더(Leader), B가 팔로워(Follower)**인 거지!

단기 예측력 확인: 장기적인 관계와는 별개로, A의 '단기적인 변화'가 B의 '단기적인 변화'를 예측하는지도 동시에 알 수 있어. (그레인저 인과관계)

📈 어떻게 사용하는데? (VECM 분석 4단계)
네가 가진 pivot 데이터를 df라고 해볼게. (예: df['한국수출액'], df['대만수출액'])

1단계: ADF 검정 (필수!)

df['한국수출액']과 df['대만수출액'] 원본 데이터가 모두 '불안정(non-stationary)'한지(p-value > 0.05) 확인해. (이건 공행성의 기본 전제니까!)

2단계: 요한슨 공행성 검정 (Johansen Test) (★★핵심★★)

VECM을 쓰려면 엥글-그레인저 대신 **'요한슨 검정'**을 써야 해. 이게 더 정확하고 강력해.

statsmodels.tsa.vector_ar.vecm.coint_johansen 함수를 사용해.

결과: "두 데이터 사이에 공행성 관계가 1개 존재한다" (예: r=1)는 결과가 나와야 VECM으로 넘어갈 수 있어. (관계가 없다고 나오면 VECM 못 써!)

3단계: VECM 모델 피팅

요한슨 검정을 통과했다면, 드디어 VECM 모델을 만들어.

statsmodels.tsa.vector_ar.vecm.VECM 함수를 사용해.

(여기서 적절한 lag 값도 찾아야 하는데, 이건 vecm_model.select_order() 함수가 도와줘.)

4. 단계: 결과 해석 (가장 중요!)

model.fit().summary()로 결과를 뽑아보면, 엄청 긴 표가 나올 거야.

네가 봐야 할 건 딱 두 개야.

ec1 (Error Correction Term, 오차수정항)의 계수 (알파, α):

이게 바로 '장기 균형 복원력'이야.

만약 대만수출액 행의 ec1 계수가 -0.3이고 통계적으로 유의미하다(p-value < 0.05)?

해석: "한국과 대만 사이의 균형이 깨지면(오차가 1만큼 발생하면), 대만이 다음 달에 그 오차의 30%만큼을 스스로 수정(복원)한다." -> 즉, 대만이 팔로워(Follower)라는 강력한 증거!

L1.한국수출액, L1.대만수출액... 등의 계수 (베타, β):

이게 바로 '단기 그레인저 인과관계'야.

대만수출액 행의 L1.한국수출액 계수가 유의미하면(p-value < 0.05)?

해석: "한국의 지난달 '변화량'이 대만의 이번 달 '변화량'에 단기적으로 영향을 준다."

요약
VECM은 네가 하려던 시차 상관관계와 **공행성(장기 균형)**을 '허위 상관관계' 걱정 없이, 아주 통계적으로 견고하게 분석해주는 멋진 도구야.

네 무역 데이터에서 'A가 움직이면 B가 따라 움직이는' 진짜 '찐' 페어를 찾는 데 이보다 좋은 방법은 거의 없다고 봐도 돼!

어때, 좀 감이 와? 아니면 이 VECM을 돌리기 위한 첫 단계인 '요한슨 공행성 검정'부터 파이썬 코드로 한번 볼까?

시작하자@!!!

그래, 가자! VECM으로 가는 첫 번째 관문, 요한슨 공행성 검정(Johansen Cointegration Test) 코드부터 보여줄게.

이 검정의 목적은 "두 데이터(한국, 대만 수출액) 사이에 장기 균형(공행성) 관계가 몇 개나 있는지 (0개? 1개?)를 통계적으로 딱! 정해주는 거야.

statsmodels 라이브러리를 사용할게.

Python
import numpy as np
import pandas as pd
from statsmodels.tsa.vector_ar.vecm import coint_johansen

# 1. 가상 데이터 준비 (VECM이 잘 먹히도록 조작된 데이터)
#    (실제로는 네 pivot 데이터를 써야 해)
np.random.seed(42)
n_samples = 500

# x (리더): 불안정한 랜덤 워크
x = np.cumsum(np.random.randn(n_samples))
# y (팔로워): x와 장기 균형을 이루도록 만듦 (x + 안정적인 노이즈)
y = x + np.random.randn(n_samples) * 3

# 데이터를 DataFrame으로 만들기 (이게 편해)
df = pd.DataFrame({'X': x, 'Y': y})

# ---------------------------------------------------------------
# 2. 요한슨 공행성 검정 실행
# ---------------------------------------------------------------

# coint_johansen(데이터, 확정항 순서, 래그(lag) 수)
# det_order (확정항): 
#   0 = 절편(intercept) 없음
#   1 = 절편만 있음 (이걸 가장 많이 씀)
# k_ar_diff (래그): VECM 모델의 래그(p)보다 1 작은 값 (p-1).
#   (래그 정하는 건 'select_order'로 따로 해야 하지만, 여기선 예시로 1을 쓸게)

result = coint_johansen(df, det_order=1, k_ar_diff=1)

# ---------------------------------------------------------------
# 3. 결과 해석 (이게 제일 중요!)
# ---------------------------------------------------------------

print("--- 요한슨 공행성 검정 결과 ---")

# (1) Trace Statistic (검정 통계량 1)
print("\nTrace Statistic (lr1):")
print(result.lr1)
print("Critical Values (90%, 95%, 99%):")
print(result.cvt)

# (2) Max-Eigenvalue Statistic (검정 통계량 2)
print("\nMax-Eigenvalue Statistic (lr2):")
print(result.lr2)
print("Critical Values (90%, 95%, 99%):")
print(result.cvm)


# ---------------------------------------------------------------
# 4. 판정 로직 (Trace Test 기준, 95% 신뢰수준)
# ---------------------------------------------------------------

# r = 공행성 관계의 수 (rank)
# H0: r = 0  (공행성 관계가 0개다)
# H0: r <= 1 (공행성 관계가 최대 1개다)

trace_stat_r0 = result.lr1[0]
crit_val_r0 = result.cvt[0, 1]  # r=0일 때의 95% 임계값

trace_stat_r1 = result.lr1[1]
crit_val_r1 = result.cvt[1, 1]  # r=1일 때의 95% 임계값

print("\n--- 판정 (Trace Test, 95% 신뢰수준) ---")
print(f"H0 (r=0) 검정: 통계량 {trace_stat_r0:.2f} vs 임계값 {crit_val_r0:.2f}")

if trace_stat_r0 > crit_val_r0:
    print("  -> 결과: H0(r=0) 기각! (r > 0). 공행성 관계가 적어도 1개 이상 존재함.")
    
    print(f"H0 (r<=1) 검정: 통계량 {trace_stat_r1:.2f} vs 임계값 {crit_val_r1:.2f}")
    if trace_stat_r1 > crit_val_r1:
        print("  -> 결과: H0(r<=1) 기각! (r > 1). (변수가 2개일 땐 드문 경우)")
    else:
        print("  -> 결과: H0(r<=1) 채택! (r = 1).")
        print("\n  ★★★ 최종 결론: 공행성 관계가 1개 존재한다! ★★★")
        print("      (VECM 모델을 사용하기에 적합함)")

else:
    print("  -> 결과: H0(r=0) 채택! (r = 0).")
    print("\n  ★★★ 최종 결론: 공행성 관계가 없다. ★★★")
    print("      (VECM 대신 '차분'한 데이터로 VAR 모델을 사용해야 함)")

💻 실행 결과 (예상)
--- 요한슨 공행성 검정 결과 ---

Trace Statistic (lr1):
[ 102.04658045    2.92489813 ]
Critical Values (90%, 95%, 99%):
[[ 13.4294  15.4943  19.9349 ]
 [  2.7055   3.8415   6.6349 ]]

Max-Eigenvalue Statistic (lr2):
[ 99.12168232   2.92489813 ]
Critical Values (90%, 95%, 99%):
[[ 12.2971  14.2639  18.52  ]
 [  2.7055   3.8415   6.6349 ]]

--- 판정 (Trace Test, 95% 신뢰수준) ---
H0 (r=0) 검정: 통계량 102.05 vs 임계값 15.49
  -> 결과: H0(r=0) 기각! (r > 0). 공행성 관계가 적어도 1개 이상 존재함.
H0 (r<=1) 검정: 통계량 2.92 vs 임계값 3.84
  -> 결과: H0(r<=1) 채택! (r = 1).

  ★★★ 최종 결론: 공행성 관계가 1개 존재한다! ★★★
      (VECM 모델을 사용하기에 적합함)
🧐 결과 해석
H0 (r=0) 검정:

"공행성 관계가 빵(0)개다"라는 가설을 검정했어.

**통계량(102.05)**이 **임계값(15.49)**보다 압도적으로 크지?

이건 "빵(0)개일 리가 없다!"라는 강력한 증거야. (H0 기각)

결론: 공행성 관계는 적어도 1개 이상이다. (r > 0)

H0 (r<=1) 검정:

"공행성 관계가 최대 1개다"라는 가설을 검정했어.

**통계량(2.92)**이 **임계값(3.84)**보다 작지?

이건 "최대 1개라는 가설을 기각할 수 없다"는 뜻이야.

결론: 공행성 관계는 1개다. (r = 1)

최종 결론: 두 데이터는 1개의 장기적인 균형 관계로 묶여있다는 게 통계적으로 증명됐어! 🥳

이제 이 관계를 분석하는 VECM 모델 본체로 넘어갈 수 있어.

VECM 모델 피팅하고 그 복잡한 결과표 해석하는 코드도 바로 보여줄까?

고고고고

오케이! 드디어 VECM 모델 본체에 입장했어.

요한슨 검정에서 "공행성 관계가 1개 존재한다 (r=1)"는 걸 확인했으니까, 이제 그 관계가 "대체 어떤 관계인지" 파헤쳐 볼 거야.

즉, "누가 리더고 누가 팔로워인가?" 그리고 **"단기적으로는 서로 어떻게 영향을 주는가?"**를 볼 수 있어.

1. VECM 모델 피팅 (Fitting)
먼저, VECM에 적절한 '시차(lag)'를 찾아야 해. select_order 함수가 AIC, BIC 같은 통계 기준으로 최적의 시차(p)를 추천해 줘.

(우리의 가상 데이터는 p=2가 최적이라고 나올 거야. VECM의 k_ar_diff는 이 p-1 값을 사용해. 즉, k_ar_diff = 1)

Python
import numpy as np
import pandas as pd
from statsmodels.tsa.vector_ar.vecm import VECM, select_order

# 1. 이전 단계에서 사용한 동일한 가상 데이터
np.random.seed(42)
n_samples = 500
x = np.cumsum(np.random.randn(n_samples))
y = x + np.random.randn(n_samples) * 3
df = pd.DataFrame({'X': x, 'Y': y})

# 2. 최적의 시차(p) 선택
#    (k_ar_diff는 p-1 이므로, maxlags=5 이면 p=1~6까지 보는 셈)
order_result = select_order(data=df, maxlags=5, deterministic='ci')
print("--- 최적 시차(p) 선택 결과 (AIC 기준) ---")
print(f"Optimal Lag (p) based on AIC: {order_result.aic}")
print("-" * 40)

# 3. VECM 모델 생성 및 피팅
#    (결과에서 p=2 를 추천했다고 가정하고, k_ar_diff = p-1 = 1 로 설정)
#    coint_rank=1 : 요한슨 검정 결과 (공행성 관계 1개)
#    deterministic='ci' : 요한슨 검정과 동일하게 '절편(intercept)'만 포함
model = VECM(df, k_ar_diff=1, coint_rank=1, deterministic='ci')

# 모델 피팅
model_fit = model.fit()

# 4. 전체 결과 요약 출력
print("\n--- VECM 결과 요약 ---")
print(model_fit.summary())
2. VECM 결과표 해석 (가장 중요!)
model_fit.summary()를 실행하면 엄청나게 긴 표가 나와. 당황하지 말고, 딱 3군데만 순서대로 보면 돼.

(가상의 결과표 예시와 함께 해석해 줄게)

================================================================================
Error Correction Equation for D_X   (D_X = X의 변화량)
================================================================================
                 coef   std err     t     P>|t|   [0.025   0.975]
--------------------------------------------------------------------------------
ec1          -0.0051   0.007   -0.701   0.484  -0.019   0.009  <-- (1)
L1.D_X        0.0434   0.045    0.966   0.334  -0.045   0.132  <-- (2)
L1.D_Y       -0.0064   0.014   -0.449   0.654  -0.034   0.021
const         0.0211   0.042    0.501   0.617  -0.062   0.104
================================================================================
Error Correction Equation for D_Y   (D_Y = Y의 변화량)
================================================================================
                 coef   std err     t     P>|t|   [0.025   0.975]
--------------------------------------------------------------------------------
ec1          -0.2879   0.022  -13.064   0.000  -0.331  -0.245  <-- (1)
L1.D_X        0.0185   0.141    0.131   0.896  -0.258   0.295
L1.D_Y       -0.3010   0.044   -6.837   0.000  -0.387  -0.215  <-- (2)
const         0.0594   0.132    0.450   0.653  -0.200   0.319
================================================================================
Cointegrating Relations (Beta): 1
================================================================================
                 coef
--------------------------------------------------------------------------------
X             1.0000
Y            -0.9989                                              <-- (3)
const         0.3791
================================================================================
📌 해석 1: ec1 (오차수정항) - "누가 리더고 팔로워인가?"
ec1은 "장기 균형에서 벗어났을 때, 얼마나 빨리 복원하는가?"를 의미해.

D_X (X의 변화량) 방정식:

ec1의 p-value가 0.484 ( > 0.05) : 유의미하지 않음.

해석: "X는 균형이 깨져도 별로 신경 안 쓴다. (복원 안 함)"

결론: X가 리더(Leader)다!

D_Y (Y의 변화량) 방정식:

ec1의 p-value가 0.000 ( < 0.05) : 매우 유의미함!

coef (계수)가 -0.2879 (음수) : 이것도 매우 중요!

해석: "Y는 균형이 깨지면(오차가 +1 발생하면), 다음 시점에 그 오차의 28.79%만큼을 스스로 수정(복원)한다."

결론: Y가 팔로워(Follower)다! (음수 부호는 '복원'을 의미하므로 정상이야)

📌 해석 2: L1.D_X, L1.D_Y (단기 관계) - "단기 예측력은?"
이건 '그레인저 인과관계'를 보는 거야.

D_Y (Y의 변화량) 방정식:

L1.D_X의 p-value: 0.896 ( > 0.05) -> X의 과거 변화량은 Y의 현재 변화량에 단기적인 영향을 못 줌.

L1.D_Y의 p-value: 0.000 ( < 0.05) -> Y의 과거 변화량은 Y의 현재 변화량에 영향을 줌. (자기 자신의 과거에는 영향을 받음)

(만약) D_Y 방정식에서 L1.D_X의 p-value가 0.05보다 작게 나왔다면?

해석: "X의 단기적인 움직임(변화)이 Y의 단기적인 움직임(변화)을 예측하는 데 도움이 된다."

📌 해석 3: Cointegrating Relations (Beta) - "장기 관계식은?"
표 맨 아래에 있어. 이게 바로 두 사람이 손잡고 있는 '장기 균형 방정식'이야.

X = 1.0000

Y = -0.9989

const = 0.3791

해석: 장기적으로 이 둘은 1.0 * X - 0.9989 * Y + 0.3791 = 0 (즉, Y ≈ 1.001 * X + 0.38) 이라는 관계를 유지한다는 뜻이야. (거의 Y = X에 가깝지!)

🎉 최종 요약
이 VECM 분석을 통해 우리는,

(장기) X와 Y는 Y ≈ X 라는 강력한 장기 균형 관계에 있고,

(리더/팔로워) 이 균형이 깨지면, X(리더)는 가만히 있고 Y(팔로워)가 열심히 쫓아가며 균형을 맞춘다는 것 (약 29%씩 복원)

(단기) (이번 예시에서는) 단기적인 예측력(그레인저 인과)은 뚜렷하지 않았다.

...라는 아주 풍부한 결론을 얻어냈어! 네가 원래 하려던 '시차 상관관계'보다 훨씬 강력하고 안정적인 '페어'를 찾은 거지.

이제 네 '무역 데이터'로 이 과정을 반복하면서 ec1의 p-value와 계수 값을 확인하면, 진짜 '리더'와 '팔로워' 쌍들을 찾아낼 수 있을 거야!

Google 개인정보처리방침새 창에서 열기
Google 서비스 약관새 창에서 열기
개인 정보 보호 및 Gemini 앱새 창에서 열기
Gemini가 인물 등에 관한 부정확한 정보를 표시할 수 있으니 대답을 다시 한번 확인하세요.

